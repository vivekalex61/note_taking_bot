{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Note taking.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpuDM57_cyAe"
      },
      "outputs": [],
      "source": [
        "#importing the modiles\n",
        "!pip install jiwer==1.2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from jiwer import wer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
        "import tarfile\n",
        "tar = tarfile.open(\"./LJSpeech-1.1.tar.bz2\", \"r:bz2\")  \n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "metadata": {
        "id": "lgjcCjTtc03k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wavs_path = \"/content/LJSpeech-1.1/wavs/\"\n",
        "metadata_path = \"/content/LJSpeech-1.1/metadata.csv\"\n",
        "# Read metadata file and parse it\n",
        "metadata_df = pd.read_csv(metadata_path, sep=\"|\", header=None, quoting=3)\n",
        "metadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\n",
        "metadata_df = metadata_df[[\"file_name\", \"normalized_transcription\"]]\n",
        "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "FJz4n83tc6SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(metadata_df)"
      ],
      "metadata": {
        "id": "e-5PiladPNi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metadata_df=metadata_df.sample(4000)\n",
        "split = int(len(metadata_df) * 0.20)\n",
        "df_train = metadata_df[:split]\n",
        "df_val = metadata_df[split:].sample(20)"
      ],
      "metadata": {
        "id": "ZaMi7rUqdFa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The set of characters accepted in the transcription.\n",
        "characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n",
        "# Mapping characters to integers\n",
        "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
        "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
        ")"
      ],
      "metadata": {
        "id": "GSkT7qkxdGa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An integer scalar Tensor. The window length in samples.\n",
        "frame_length = 256\n",
        "# An integer scalar Tensor. The number of samples to step.\n",
        "frame_step = 160\n",
        "# An integer scalar Tensor. The size of the FFT to apply.\n",
        "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
        "fft_length = 384\n",
        "\n",
        "\n",
        "def encode_single_sample(wav_file, label):\n",
        "    ###########################################\n",
        "    ##  Process the Audio\n",
        "    ##########################################\n",
        "    # 1. Read wav file\n",
        "    file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n",
        "    # 2. Decode the wav file\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    # 3. Change type to float\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "    # 4. Get the spectrogram\n",
        "    spectrogram = tf.signal.stft(\n",
        "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "    )\n",
        "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "    # 6. normalisation\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        "    ###########################################\n",
        "    ##  Process the label\n",
        "    ##########################################\n",
        "    # 7. Convert label to Lower case\n",
        "    label = tf.strings.lower(label)\n",
        "    # 8. Split the label\n",
        "    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
        "    # 9. Map the characters in label to numbers\n",
        "    label = char_to_num(label)\n",
        "    # 10. Return a dict as our model is expecting two inputs\n",
        "    return spectrogram, label"
      ],
      "metadata": {
        "id": "rXnnqinDdIvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "# Define the trainig dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (list(df_train[\"file_name\"]), list(df_train[\"normalized_transcription\"]))\n",
        ")\n",
        "train_dataset = (\n",
        "    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Define the validation dataset\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (list(df_val[\"file_name\"]), list(df_val[\"normalized_transcription\"]))\n",
        ")\n",
        "validation_dataset = (\n",
        "    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "RjQwWzRudK70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "amS8DuhL_XK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8, 5))\n",
        "for batch in train_dataset.take(1):\n",
        "    spectrogram = batch[0][0].numpy()\n",
        "    spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n",
        "    label = batch[1][0]\n",
        "    # Spectrogram\n",
        "    label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "    ax = plt.subplot(2, 1, 1)\n",
        "    ax.imshow(spectrogram, vmax=1)\n",
        "    ax.set_title(label)\n",
        "    ax.axis(\"off\")\n",
        "    # Wav\n",
        "    file = tf.io.read_file(wavs_path + list(df_train[\"file_name\"])[0] + \".wav\")\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = audio.numpy()\n",
        "    ax = plt.subplot(2, 1, 2)\n",
        "    plt.plot(audio)\n",
        "    ax.set_title(\"Signal Wave\")\n",
        "    ax.set_xlim(0, len(audio))\n",
        "    display.display(display.Audio(np.transpose(audio), rate=16000))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E618sMrwdtMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CTCLoss(y_true, y_pred):\n",
        "    # Compute the training-time loss value\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "R1L4Za3Jdt7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n",
        "    \"\"\"Model similar to DeepSpeech2.\"\"\"\n",
        "    # Model's input\n",
        "\n",
        "    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
        "    # Expand the dimension to use 2D CNN.\n",
        "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
        "    # Convolution layer 1\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 41],\n",
        "        strides=[2, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_1\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
        "    # Convolution layer 2\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 21],\n",
        "        strides=[1, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_2\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
        "    # Reshape the resulted volume to feed the RNNs layers\n",
        "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
        "    # RNN layers\n",
        "    '''for i in range(1, rnn_layers + 1):\n",
        "        recurrent = layers.GRU(\n",
        "            units=rnn_units,\n",
        "            activation=\"tanh\",\n",
        "            recurrent_activation=\"sigmoid\",\n",
        "            use_bias=True,\n",
        "            return_sequences=True,\n",
        "            reset_after=True,\n",
        "            name=f\"gru_{i}\",\n",
        "        )\n",
        "        x = layers.Bidirectional(\n",
        "            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
        "        )(x)\n",
        "        if i < rnn_layers:\n",
        "            x = layers.Dropout(rate=0.5)(x)\n",
        "    '''\n",
        "    recurrent = layers.GRU(\n",
        "            units=rnn_units,\n",
        "            activation=\"tanh\",\n",
        "            recurrent_activation=\"sigmoid\",\n",
        "            use_bias=True,\n",
        "            return_sequences=True,\n",
        "            reset_after=True,\n",
        "            name=\"gru1\",\n",
        "        )(x)\n",
        "    x = layers.Dropout(rate=0.5)(recurrent)    \n",
        "    recurrent2 = layers.GRU(\n",
        "            units=rnn_units,\n",
        "            activation=\"tanh\",\n",
        "            recurrent_activation=\"sigmoid\",\n",
        "            use_bias=True,\n",
        "            return_sequences=True,\n",
        "            reset_after=True,\n",
        "            name=\"gru2\",\n",
        "        )(x)\n",
        "    # Dense layer\n",
        "    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(recurrent2)\n",
        "    x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
        "    x = layers.Dropout(rate=0.5)(x)\n",
        "    # Classification layer\n",
        "    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
        "    # Model\n",
        "    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
        "    # Optimizer\n",
        "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt, loss=CTCLoss)\n",
        "    return model\n",
        "\n",
        "def bidirectional_rnn_model(input_dim, units, output_dim):\n",
        "    \"\"\" Build a bidirectional recurrent network for speech\n",
        "    \"\"\"\n",
        "    # Main acoustic input\n",
        "    input_data =  layers.Input(name='the_input', shape=(None, input_dim))\n",
        "    # TODO: Add bidirectional recurrent layer\n",
        "    bidir_rnn = layers.Bidirectional(layers.LSTM(units, return_sequences=True, activation='relu'), merge_mode='concat')(input_data)\n",
        "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
        "    time_dense = layers.TimeDistributed(layers.Dense(output_dim+1))(bidir_rnn)\n",
        "    # Add softmax activation layer\n",
        "    y_pred = layers.Activation('softmax', name='softmax')(time_dense)\n",
        "    # Specify the model\n",
        "    model = keras.Model(inputs=input_data, outputs=y_pred)\n",
        "    #model.output_length = lambda x: x\n",
        "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt, loss=CTCLoss)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Get the model\n",
        "model = build_mode(\n",
        "    input_dim=fft_length // 2 + 1,\n",
        "    output_dim=char_to_num.vocabulary_size()\n",
        "#    rnn_units=512,\n",
        ")\n",
        "model.summary(line_length=110)"
      ],
      "metadata": {
        "id": "eUuw6Jt6dwF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    # Use greedy search. For complex tasks, you can use beam search\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
        "    # Iterate over the results and get back the text\n",
        "    output_text = []\n",
        "    for result in results:\n",
        "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
        "        output_text.append(result)\n",
        "    return output_text\n",
        "\n",
        "\n",
        "# A callback class to output a few transcriptions during training\n",
        "class CallbackEval(keras.callbacks.Callback):\n",
        "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs=None):\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        for batch in self.dataset:\n",
        "            X, y = batch\n",
        "            batch_predictions = model.predict(X)\n",
        "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "            predictions.extend(batch_predictions)\n",
        "            for label in y:\n",
        "                label = (\n",
        "                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "                )\n",
        "                targets.append(label)\n",
        "        wer_score = wer(targets, predictions)\n",
        "        print(\"-\" * 100)\n",
        "        print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "        print(\"-\" * 100)\n",
        "        for i in np.random.randint(0, len(predictions), 2):\n",
        "            print(f\"Target    : {targets[i]}\")\n",
        "            print(f\"Prediction: {predictions[i]}\")\n",
        "            print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "wZRqxMocdy1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of epochs.\n",
        "epochs = 45\n",
        "# Callback function to check transcription on the val set.\n",
        "validation_callback = CallbackEval(validation_dataset)\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[validation_callback],\n",
        ")\n"
      ],
      "metadata": {
        "id": "RhqhpiCcd3wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "targets = []\n",
        "for batch in validation_dataset:\n",
        "    X, y = batch\n",
        "    print(X.shape)\n",
        "    batch_predictions = model.predict(X)\n",
        "    batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "    predictions.extend(batch_predictions)\n",
        "    for label in y:\n",
        "        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "        targets.append(label)\n",
        "wer_score = wer(targets, predictions)\n",
        "print(\"-\" * 100)\n",
        "print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "print(\"-\" * 100)\n",
        "for i in np.random.randint(0, len(predictions), 5):\n",
        "    print(f\"Target    : {targets[i]}\")\n",
        "    print(f\"Prediction: {predictions[i]}\")\n",
        "    print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "qXv6l2U-mLXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('my_model.h5') "
      ],
      "metadata": {
        "id": "Fqkd7rLjLR-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kOgCD38GOb1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediciton"
      ],
      "metadata": {
        "id": "f0zkHCPeOcMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install pydub\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "import pyaudio\n",
        "import wave\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "#preprocessing\n",
        "# An integer scalar Tensor. The window length in samples.\n",
        "frame_length = 256\n",
        "# An integer scalar Tensor. The number of samples to step.\n",
        "frame_step = 160\n",
        "# An integer scalar Tensor. The size of the FFT to apply.\n",
        "# If not provided, uses the smallest power of 2 enclosing frame_length.\n",
        "fft_length = 384\n",
        "# The set of characters accepted in the transcription.\n",
        "characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n",
        "# Mapping characters to integers\n",
        "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        ")\n",
        "def record():\n",
        "\n",
        "            FORMAT = pyaudio.paInt16\n",
        "            CHANNELS = 1\n",
        "            RATE = 16000\n",
        "            CHUNK = 512\n",
        "            RECORD_SECONDS = 10\n",
        "            WAVE_OUTPUT_FILENAME = \"recordedFile.wav\"\n",
        "            device_index = 2\n",
        "            audio = pyaudio.PyAudio()\n",
        "\n",
        "            print(\"----------------------record device list---------------------\")\n",
        "            info = audio.get_host_api_info_by_index(0)\n",
        "            numdevices = info.get('deviceCount')\n",
        "            for i in range(0, numdevices):\n",
        "                    if (audio.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:\n",
        "                        print(\"Input Device id \", i, \" - \", audio.get_device_info_by_host_api_device_index(0, i).get('name'))\n",
        "\n",
        "            print(\"-------------------------------------------------------------\")\n",
        "\n",
        "            index = int(input())\n",
        "            print(\"recording via index \"+str(index))\n",
        "\n",
        "            stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
        "                            rate=RATE, input=True,input_device_index = index,\n",
        "                            frames_per_buffer=CHUNK)\n",
        "            print (\"recording started\")\n",
        "            Recordframes = []\n",
        "            \n",
        "            for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
        "                data = stream.read(CHUNK)\n",
        "                Recordframes.append(data)\n",
        "            print (\"recording stopped\")\n",
        "            \n",
        "            stream.stop_stream()\n",
        "            stream.close()\n",
        "            audio.terminate()\n",
        "\n",
        "            waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
        "            waveFile.setnchannels(CHANNELS)\n",
        "            waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
        "            waveFile.setframerate(RATE)\n",
        "            waveFile.writeframes(b''.join(Recordframes))\n",
        "            waveFile.close()\n",
        "\n",
        "\n",
        "def chunk_audio(file_name):\n",
        "    #reading from audio mp3 file\n",
        "    sound = AudioSegment.from_mp3(file_name)\n",
        "    # spliting audio files\n",
        "    audio_chunks = split_on_silence(sound, min_silence_len=200, silence_thresh=-40 )\n",
        "    list_of_chunk_files=[]\n",
        "    #loop is used to iterate over the output list\n",
        "    for i, chunk in enumerate(audio_chunks):\n",
        "      output_file = \"chunk{0}.wav\".format(i)\n",
        "      list_of_chunk_files.append(\"/content/chunk{0}.wav\".format(i))\n",
        "      print(\"Exporting file\", output_file)\n",
        "      chunk.export(output_file, format=\"wav\")\n",
        "    return list_of_chunk_files\n",
        "\n",
        "def writing_file(predictions):\n",
        "            predictions=\" \".join(predictions)\n",
        "            # Program to show various ways to read and\n",
        "            # write data in a file.\n",
        "            file1 = open(\"Notes.txt\",\"w\")\n",
        "\n",
        "            file1.writelines(predictions)\n",
        "            file1.close() #to change file access modes\n",
        "       \n",
        "def pre_single_sample(wav_file):\n",
        "    ###########################################\n",
        "    ##  Process the Audio\n",
        "    ##########################################\n",
        "    # 1. Read wav file\n",
        "    file = tf.io.read_file(wav_file)\n",
        "    # 2. Decode the wav file\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    # 3. Change type to float\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "    # 4. Get the spectrogram\n",
        "    spectrogram = tf.signal.stft(\n",
        "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "    )\n",
        "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "    # 6. normalisation\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        " \n",
        "    return spectrogram\n",
        "# A utility function to decode the output of the network\n",
        "\n",
        "def decode_batch_predictions(pred):\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    # Use greedy search. For complex tasks, you can use beam search\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
        "    # Iterate over the results and get back the text\n",
        "    output_text = []\n",
        "    for result in results:\n",
        "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
        "        output_text.append(result)\n",
        "    return output_text\n",
        "\n",
        "def predictions(file_list,model):\n",
        "        predictions = []\n",
        "        pred_data = tf.data.Dataset.from_tensor_slices(\n",
        "            (file_list)\n",
        "        )\n",
        "        pred_dataset = (\n",
        "          pred_data.map(pre_single_sample,num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            .padded_batch(1)\n",
        "            .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        )\n",
        "        for batch in pred_dataset:  \n",
        "            X = batch\n",
        "            batch_predictions = model.predict(X)\n",
        "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "            predictions.extend(batch_predictions)\n",
        "        return predictions            \n"
      ],
      "metadata": {
        "id": "JoeBsglUmMbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6YepCr0tOaf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=load_model('/content/my_model.h5',custom_objects={'CTCLoss': CTCLoss})\n",
        "record()\n",
        "fil=chunk_audio('/content/recordedFile.wav')\n",
        "predictions= predictions(fil,model)\n",
        "writing_file(predictions)"
      ],
      "metadata": {
        "id": "hSeHYPS7lUBn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}